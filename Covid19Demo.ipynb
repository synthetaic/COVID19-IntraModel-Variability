{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetaic COVID-19 CNN Demo\n",
    "\n",
    "As an artificial intelligence technology company, we feel it's our duty to provide datasets, AI tools, and predictive analytics that are relevant to the current COVID-19 crisis. Our aims are to catalyze other AI and modeling developments, help researchers analyze the large body of available research, and share visualizations with the general public. If the public is equipped with the accurate information about the spread of COVID-19, we are convinced that many will take action to prevent its spread.\n",
    "\n",
    "Disclaimer: This tool is for research and educational purposes only and is not intended to be a tool for decision-making. There are many uncertainties and debates about the details of COVID-19 infection and transmission and there are many limitations in any modeling approach. Please talk to your primary care physician if you experience symptoms of COVID-19. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZPdl5B_DN4I"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, autograd, optim, utils  \n",
    "from torch.nn import functional as F\n",
    "from torch.nn import parallel\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "\n",
    "import kornia\n",
    "from kornia.losses import FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-IdPzm4BDZHJ"
   },
   "source": [
    "# Global Variables Initializer \n",
    "\n",
    "### Project-specific variables<br>\n",
    "`var_proj_name` - Project name for the neural network.<br>\n",
    "`var_arch` - Neural Network architecture to use. Spelling sensitive. {resnet18, resnet50, resnet101, densenet121, densenet169, resnext50_32x4d, mobilenet_v2}<br>\n",
    "`var_size` - Input size dimension of the data. Must be size x size and Inception V3 requires 299x299 default: 224x224<br>\n",
    "`var_dataset_path` - Path to chest scan dataset folder. Folder must be set up like Pytorch's ImageFolder. default: './data/xrayscans/'<br>\n",
    "\n",
    "### Session-specific variables<br>\n",
    "`var_ckp` - Which checkpoint to continue training from or test. default: None<br>\n",
    "`var_start_iter` - What iteration to continue from if training is continued. default: 0<br>\n",
    "`var_iter` - How many total training iteration or epochs to perform if early stopping does not occur. default: 100<br>\n",
    "`var_batch_size` - Batch size. default: 64<br>\n",
    "`var_output` - The number of lung diagnosis classs {COVID-19, normal, pneumonia}. default: 3<br>\n",
    "`var_optimizer` - Optimizer to use {adam, sgd, rmsprop}. default: adam<br>\n",
    "\n",
    "#### Focal Loss specific variables <br>\n",
    "`var_use_focal` - Set to True if focal loss is to be used. default: False <br>\n",
    "`var_alpha` - Alpha hyperparameter for Focal Loss. default: 0.5 <br>\n",
    "`var_gamma` - Gamma hyperparameter for Focal Loss. default: 2<br>\n",
    "`var_reduction` - Reduction method for batch losses. default: mean<br>\n",
    "\n",
    "#### Staged-Transfer Training Protocol Specific variables\n",
    "`var_use_staged_tt` - Set to True if multiple stages of unfreezing or traditional fitting of last FC layer. default: True <br>\n",
    "`var_stage_limit` - Maximum amount of stages to occur. Limit may not be met due to early stopping or network size is too small. default: 3 <br>\n",
    "`var_stage_plateau` - How many iterations to wait for improvements to valid loss until next stage of transfer training occurs. default: 10 <br>\n",
    "`var_network_ratio` - Ratio of the network to unfreeze after every stage. Unfreezes from the last unfrozen layer from the end. `var_network_ratio * var_stage_limit` must be equal to or less than 1.0. default: 0.1 <br>\n",
    "`var_use_change_factor` - Set to True if change factor is to be used as the updating learning rate factor. `change_factor = var_lr_factor ^ cur_stage` default: True<br>\n",
    "`var_change_style` - Method to update the learning rate factor at each stage {compounding, steady} `current_learning_rate = change_factor * or + current_learning_rate`. default: compounding<br>\n",
    "`var_lr_factor` - The factor to update the learning rate. `LR = LRF * LR` where LRF is `change_factor * stage_count * var_lr_factor` default: 0.1<br>\n",
    "`var_lr` - Learning rate for training. default: 0.002 <br>\n",
    "`var_weight_decay` - Weight decay for training. default: 1e-4<br>\n",
    "`var_reduce_lr` - Set to True if a learning rate scheduler is to be implement. Reduces learning rate after `var_lr_plateau` iterations have occured without a lower loss.<br>\n",
    "`var_lr_plateau` - Plateau patience value for learning rate scheduler. <br>\n",
    "\n",
    "__Traditional Transfer Training where all weights are frozen except for the last FC layer and fitted, and then all weights are unfrozen and network is fitted options are available by toggling `var_use_staged_tt` to False.__\n",
    "\n",
    "### System-specific variables<br>\n",
    "`var_device` - 'cuda' if using gpu or 'cpu' if using cpu. default:'gpu'<br>\n",
    "`var_gpus` - List of gpu ids to use for data parallel training. default: [0,1]<br>\n",
    "`var_distributed` - Feature requirement enables distributed training. default: False<br>\n",
    "`is_notebook` - Boolean to be used when this notebook is in python scirpt version.\n",
    "\n",
    "\n",
    "### Directory should appear like this.\n",
    "```\n",
    "Demo\n",
    "|___Covid19Demo.ipynb\n",
    "|___data\n",
    "    |___xrayscans\n",
    "        |___train\n",
    "        |   |____pneumonia\n",
    "        |   |____normal\n",
    "        |   |____COVID-19\n",
    "        |____test\n",
    "             |____pneumonia\n",
    "             |____normal\n",
    "             |____COVID-19\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kltee6D6DNAw"
   },
   "outputs": [],
   "source": [
    "# Project specific variables\n",
    "var_proj_name = 'Default'\n",
    "var_arch = 'resnet18'\n",
    "\n",
    "var_size = 299 if var_arch == 'inception_v3' else 512\n",
    "var_dataset_path = '~/data/data_v3/'\n",
    "var_output = 3\n",
    "\n",
    "\n",
    "# Session specific variables\n",
    "# var_ckpt = '/data/AAA_Corona/Code/Git/covidresnet18ckptsV1/resnet18v3/32model_bestLOSS0_2267882215338237.pt'\n",
    "var_ckpt = None\n",
    "var_start_iter = 0\n",
    "var_iter = 10\n",
    "var_batch_size = 64\n",
    "var_optimizer = 'adam'\n",
    "\n",
    "var_use_focal = False\n",
    "var_alpha = 0.5\n",
    "var_gamma = 5\n",
    "var_reduction ='mean'\n",
    "\n",
    "# Staged-Transfer Training specific variables\n",
    "var_use_staged_tt = False\n",
    "var_stage_limit = 3\n",
    "var_stage_plateau = 10\n",
    "var_network_ratio = 0.1\n",
    "var_use_change_factor = True\n",
    "var_change_style = 'compounding'\n",
    "var_lr_factor = 0.1\n",
    "\n",
    "# # Traditional Transfer Training specific variables (Uncomment to use)\n",
    "# var_use_staged_tt = False\n",
    "# var_stage_limit = 1\n",
    "# var_stage_plateau = 1\n",
    "# var_network_ratio = 1.0\n",
    "# var_use_change_factor = True\n",
    "# var_change_style = 'compounding'\n",
    "# var_lr_factor = 0.1\n",
    "\n",
    "\n",
    "var_lr = 5e-4\n",
    "var_weight_decay = 1e-4\n",
    "var_reduce_lr = True\n",
    "var_stage_plateau = 1\n",
    "var_lr_plateau = 5\n",
    "var_end_plateau = 1\n",
    "\n",
    "# System specific variables\n",
    "var_device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "var_gpus = [0,1]\n",
    "var_distributed = False\n",
    "\n",
    "is_notebook = True\n",
    "# Append miscelanneous variables\n",
    "# var_eval = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Python Script\n",
    "This jupyter was converted to a python file in order to be ran on a server with terminal arguments. This is helpful when for example hyperparameter searching requires many iterations to narrow down the best options. Editing arguments in the terminal is easier than editing code in jupyter notebook. <br>\n",
    "To convert this notebook to a python script proceed with the following steps:<br>\n",
    "- Comment the cell above which initializes the project's settings.\n",
    "- Uncomment the cell above which enables the argument parser and sets the is_script to True which will remove GUI plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add('data', default='./data_folder', type=str, help='Path to the folder containing the training images.')\n",
    "#     parser.add('--proj-name', default='default', type=str, help='Project name for the neural network.')\n",
    "#     parser.add('--arch', default='resnet18',  type=str, help='Neural network architecture to choose from {resnet18, resnet50, resnet101, densenet121, densenet169, resnext50_32x4d, mobilenet_v2, inception_v3}')\n",
    "#     parser.add('--size', default=512, type=int, help='Size of the input image. Must be square (height == width)')\n",
    "#     parser.add('--output', default=3, type=int, help='Number of output classes for transfer training. {COVID-19, normal, pneumonia}')\n",
    "#     parser.add('--evaluate', default=False, action='store_true', help='Use network evaluation mode.')\n",
    "#     parser.add('--ckpt', default=None, type=str, help='Path to network checkpoint for metric evaluation.')\n",
    "#     parser.add('--iters',  default=200, type=int, help='Number of iterations to train for.')\n",
    "#     parser.add('--start-iter', default=0, type=int, help='Iteration to start at. For continued training.')\n",
    "#     parser.add('--batch-size', default=64, type=int, help='Batch size for training and evaluating.')\n",
    "#     parser.add('--optimizer', default'adam', type=str, help='Optimizer to use for training. {sgd, adam, rmsprop}')\n",
    "#     parser.add('--lr', default=0.0002, type=float, help='Learning Rate for training.')\n",
    "#     parser.add('--weight-decay', default=1e-4, type=float, help='Weight Decay for training.')\n",
    "#     parser.add('--gpus', default='0', type=str, help='String of GPUs to use deliminated by a comma. Default is to use all available GPUs. IE: 2 GPUs 3,4 --gpus=\"3,4\"')\n",
    "#     parser.add('--distributed', default=False, action='store_true')\n",
    "    \n",
    "#     parser.add('--use-focal', default=False, action='store_true', help='Use focal loss. Alpha, Gamma, and reduction method can be set.')\n",
    "#     parser.add('--focal-alpha', default=0.5, type=float, help='Alpha hyperparameter for focal loss.')\n",
    "#     parser.add('--focal-gamma', default=2, type=int, help='Gamma hyperparameter for focal loss.')\n",
    "#     parser.add('--focal-reduction', default='mean', type=str, help='Reduction method to apply for batch loss. {none, mean, sum}')\n",
    "    \n",
    "#     parser.add('--use_staged_tt', default=False, action='store_true', help='Use Staged Transfer Training Protocol.')\n",
    "#     parser.add('--stage-limit', default=3, type=int, help='Total count of stages to unfreeze for transfer training.')\n",
    "#     parser.add('--plateau-patience', default=10, type=int, help='Number of iterations without lower valid loss until moving to the next stage of transfer training.')\n",
    "#     parser.add('--network-ratio', default=0.2, type=float, help='Percentage of the network to unfreeze at every stage.')\n",
    "#     parser.add('--change-factor', default='exponential', type=str, help='Method to update the change factor as the stages progress. {exponential, linear}')\n",
    "#     parser.add('--change-style', default='compounding', type=str, help='Method to update the training learning rate. {compounding, steady}')\n",
    "#     parser.add('--lr-factor', default=0.1, type=float, help='Base hyperparameter lr factor.  ')\n",
    "    \n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "\n",
    "# # Project specific variables\n",
    "# var_proj_name = args.proj_name\n",
    "# var_arch = args.arch\n",
    "# var_size = 299 if var_arch == 'inception_v3' else args.size\n",
    "# var_dataset_path = args.data\n",
    "# var_output = args.output\n",
    "\n",
    "\n",
    "# # Session specific variables\n",
    "# # var_ckpt = '/data/AAA_Corona/Code/Git/COVID19-Chest-xrays/RippedDiscriminator/checkpoints/resnet18v5/3model_bestLOSS0_262ACC0_88.pt'\n",
    "# var_ckpt = args.ckpt\n",
    "# var_start_iter = args.start_iter\n",
    "# var_iter = args.iters\n",
    "# var_batch_size = args.batch_size\n",
    "# var_lr = args.lr\n",
    "# var_weight_decay = args.weight_decay\n",
    "# var_optimizer = args.optimizer\n",
    "# var_use_focal = args.use_focal\n",
    "# if var_use_focal:\n",
    "#     var_alpha = args.focal_alpha\n",
    "#     var_gamma = args.focal_gamma\n",
    "#     reduction_methods = ['none', 'mean', 'sum']\n",
    "#     var_reduction = args.focal_reduction if args.focal_reduction in reduction_methods else 'mean'\n",
    "\n",
    "# # Transfer Training specific variables\n",
    "# var_use_staged_tt = args.use_staged_tt\n",
    "# var_stage_limit = 3\n",
    "# var_stage_plateau = 10\n",
    "# var_network_ratio = 0.2\n",
    "# var_change_factor = 'exponential'\n",
    "# var_change_style = 'compounding'\n",
    "# var_lr_factor = .1\n",
    "    \n",
    "# # System specific variables\n",
    "# var_gpus = [int(gpu) for gpu in var.gpus.split(',')]\n",
    "# var_device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# var_distributed = False\n",
    "\n",
    "# is_notebook = False\n",
    "# # Append miscelanneous variables\n",
    "# # var_eval = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Project folders\n",
    "Automatically checks whether there is a project with the `var_proj_name` and creates the appropriate project directory if there exists none, else it modifies the `var_proj_name` by appending an underscore and a digit. IE: `proj_1`<br>\n",
    "\n",
    "\n",
    "This means `var_proj_name` cannot contain an underscore. This is so a previous runs checkpoints are accidently overwritten.<br><br>\n",
    "\n",
    "Creates the subfolders used for storing metrics and logging.\n",
    "<br>\n",
    "Here are what each project subfolders contains.<br>\n",
    "- `cms` : Holds image plots of the confusion matrix of the test set.<br>\n",
    "- `graphs` : Holds image plots of the accuracy and loss for training and validation sets. <br>\n",
    "- `logs` : Contains the tensorboard log files which can be opened up in Tensorboard to analyze. <br>\n",
    "\n",
    "### Directory should appear like this.\n",
    "```\n",
    "Demo\n",
    "|___Covid19Demo.ipynb\n",
    "|___data\n",
    "|___checkpoints\n",
    "    |___proj_1\n",
    "    |   |___cms\n",
    "    |   |___graphs\n",
    "    |   |___logs\n",
    "    |\n",
    "    |___proj_2\n",
    "        |___cms\n",
    "        |___graphs\n",
    "        |___logs\n",
    "```\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "ckpt_dir = os.path.join(cur_dir, 'checkpoints_fin')\n",
    "if os.path.exists(ckpt_dir) is False:\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    \n",
    "while os.path.exists(os.path.join(ckpt_dir, var_proj_name)):\n",
    "    proj_ver = var_proj_name.split('_')\n",
    "    if len(proj_ver) == 1:\n",
    "        var_proj_name += '_1'\n",
    "    else:\n",
    "        next_ver = str(int(proj_ver[1]) + 1)\n",
    "        var_proj_name = '{}_{}'.format(proj_ver[0], next_ver)\n",
    "        \n",
    "proj_ckpt = os.path.join(ckpt_dir, var_proj_name)\n",
    "if os.path.exists(proj_ckpt) is False:\n",
    "    os.makedirs(proj_ckpt, exist_ok=True)\n",
    "    \n",
    "log_dir = os.path.join(proj_ckpt, 'logs')\n",
    "if os.path.exists(log_dir) is False:\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "cm_dir = os.path.join(proj_ckpt, 'cms')\n",
    "if os.path.exists(cm_dir) is False:\n",
    "    os.makedirs(cm_dir, exist_ok=True)\n",
    "    \n",
    "graph_dir = os.path.join(proj_ckpt, 'graphs')\n",
    "if os.path.exists(graph_dir) is False:\n",
    "    os.makedirs(graph_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0aWAg1WvEFz1"
   },
   "source": [
    "## Data Utility Helper Function Set Up\n",
    "Some helper functions that will help later.<br>\n",
    "Initialize Tensorboard for logging of loss and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8kTWRFuHEQ2v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czhong/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/czhong/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/czhong/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/czhong/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/czhong/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/czhong/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "def data_sampler(dataset, shuffle, distributed):\n",
    "    if distributed:\n",
    "        return data.distributed.DistributedSampler(dataset, shuffle=shuffle)\n",
    "\n",
    "    if shuffle:\n",
    "        return data.RandomSampler(dataset)\n",
    "    else:\n",
    "        return data.SequentialSampler(dataset)\n",
    "\n",
    "# Create directory for tensorboard logs.\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEH85DT8FwEA"
   },
   "source": [
    "# Data Preprocessing\n",
    "The often-updated COVID-19 dataset consists of pneumonia, normal, and COVID-19 chest xray scans from AP and PA views.<br>\n",
    "\n",
    "Images are resized and square cropped. The samples are then normalized according to Imagenet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation and Transformations.\n",
    "In order to compensate for the small number of training samples, it has been found to be helpful to add data augmentation to help the training process. <br><br>\n",
    "Initially, default transformations are simple resize and crop. <br>\n",
    "Normalization must be done because the pretrained weights were trained on imagenet normalized to the specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((var_size, var_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=(0.9, 1.1)),\n",
    "        transforms.RandomAffine(degrees=10,translate=(0.1,0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),    \n",
    "    ])\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((var_size, var_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),    \n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((var_size, var_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets\n",
    "From the test and train subfolders of the `var_dataset_path` variable, create their respective Pytorch dataset class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/czhong/data/data_v3/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-97ca8586d56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/czhong/.local/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/czhong/.local/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     91\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     92\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/czhong/.local/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/czhong/data/data_v3/train'"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(var_dataset_path, 'train')\n",
    "val_path = os.path.join(var_dataset_path, 'val')\n",
    "test_path = os.path.join(var_dataset_path, 'test')\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_path, transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(val_path, transform=val_transform)\n",
    "test_dataset = datasets.ImageFolder(test_path, transform=test_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pytorch Dataloader\n",
    "Create a dataloader for each of the datasets. <br>\n",
    "Default implementation does not take advantage of multiprocessing. In order to take advantage of multiprocessing and the requirements are met, then do not run the next cell code. <br>\n",
    "Comment the next cell, and uncomment the cell after the next cell to fully use multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "1PzZ6RvoGSfx",
    "outputId": "49c6621f-6af7-4d24-f67c-912e1f163d33"
   },
   "outputs": [],
   "source": [
    "# train_dataloader = data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=var_batch_size,\n",
    "#     sampler=data_sampler(train_dataset, shuffle=True, distributed=var_distributed),\n",
    "# )\n",
    "\n",
    "# val_dataloader = data.DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=var_batch_size,\n",
    "#     sampler=data_sampler(val_dataset, shuffle=False, distributed=var_distributed),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS CODE if multiple workers are to be used for dataloaders.\n",
    "\n",
    "workers = math.floor(mp.cpu_count() * 0.75) if mp.cpu_count() >= 2 else 1\n",
    "print('Using {} workers for dataloaders.'.format(str(workers)))\n",
    "\n",
    "# The order in which the dataloader presents to the network the training data is one source of randomness which separates this run from other runs.\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=var_batch_size,\n",
    "    sampler=data_sampler(train_dataset, shuffle=True, distributed=var_distributed),\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=var_batch_size,\n",
    "    sampler=data_sampler(val_dataset, shuffle=True, distributed=var_distributed),\n",
    "    num_workers=workers,\n",
    ")\n",
    "\n",
    "test_dataloader = data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=var_batch_size,\n",
    "    sampler=data_sampler(test_dataset, shuffle=True, distributed=var_distributed),\n",
    "    num_workers=workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Scan Images of Training, Val, and Test sets\n",
    "View a small subsample of your training and test set to observe what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to return key for any value \n",
    "def get_key(val): \n",
    "    for key, value in train_dataset.class_to_idx.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "        \n",
    "# function to obtain label counts\n",
    "def dataset_distribution(dataset):\n",
    "    dist = {'COVID-19':0,'normal':0,'pneumonia':0}\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        if get_key(dataset[i][1]) == \"COVID-19\":\n",
    "            dist['COVID-19'] +=1\n",
    "        elif get_key(dataset[i][1]) == \"normal\":\n",
    "            dist['normal'] +=1\n",
    "        else:    \n",
    "            dist['pneumonia'] +=1\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print out some data stats\n",
    "# print('Number of training images: ', len(train_dataset))\n",
    "# print('Number of validation images: ', len(val_dataset))   \n",
    "# print('Number of testing images: ', len(test_dataset))   \n",
    "\n",
    "# print('Number of classes: ', len(train_dataset.classes))\n",
    "# print('Name of classes: ', train_dataset.classes)\n",
    "# print('Labels: ', train_dataset.class_to_idx)\n",
    "\n",
    "# train_dist = dataset_distribution(train_dataset)\n",
    "# print('Training set Distribution: {}'.format(train_dist))\n",
    "# val_dist = dataset_distribution(val_dataset)\n",
    "# print('Validation set Distribution: {}'.format(val_dist))\n",
    "# test_dist = dataset_distribution(test_dataset)\n",
    "# print('Testing set Distribution: {}'.format(test_dist))\n",
    "\n",
    "# # Analyzing the shape of one batch\n",
    "# train_images, train_labels = next(iter(train_dataloader))\n",
    "# val_images, val_labels = next(iter(val_dataloader))\n",
    "# test_images, test_labels = next(iter(test_dataloader))\n",
    "\n",
    "# print('Shape of first batch of training images: ', train_images.shape)\n",
    "# print('Shape of first batch of validation images: ', val_images.shape)\n",
    "# print('Shape of first batch of testing images: ', test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "if is_notebook:\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    for idx in np.arange(10):\n",
    "        ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "        plt.imshow(images[idx][0].numpy().squeeze(), cmap='bone')\n",
    "        ax.set_title(train_dataset.classes[int(labels[idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of VAL images\n",
    "dataiter = iter(val_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "if is_notebook:\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    for idx in np.arange(10):\n",
    "        ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "        plt.imshow(images[idx][0].numpy().squeeze(), cmap='bone')\n",
    "        ax.set_title(val_dataset.classes[int(labels[idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of TEST images\n",
    "dataiter = iter(test_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "if is_notebook:\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    for idx in np.arange(10):\n",
    "        ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "        plt.imshow(images[idx][0].numpy().squeeze(), cmap='bone')\n",
    "        ax.set_title(test_dataset.classes[int(labels[idx])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0KSR5gxJ2W8"
   },
   "source": [
    "## Model Utility Helper Function Set Up\n",
    "Create functions which will help save time later. <br><br>\n",
    "`get_named_model(arch, pretrained)` : Helper function to encapsulate switch-like behavior for architecture choice. <br><br>\n",
    "`modify_model(model, arch, channels)` : Helper function to help modify model for transfer training on new objective. <br>\n",
    "\n",
    "- Transfer training involves freezing the learned weights of a pretrained networks' convolution filters.<br>\n",
    "\n",
    "- As well as replacing the last classifier or fully connected layer with a new classifier or fully connected layer.<br>\n",
    "\n",
    "- These weights are not frozen. By default, initializing layers automatically sets `requires_grad` to true.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33e74xwJKCX-"
   },
   "outputs": [],
   "source": [
    "def get_named_model(arch, pre):\n",
    "    if arch == 'resnet18':\n",
    "        return models.resnet18(pretrained=pre)\n",
    "    elif arch == 'resnet50':\n",
    "        return models.resnet50(pretrained=pre)\n",
    "    elif arch == 'resnet101':\n",
    "        return models.resnet101(pretrained=pre)\n",
    "    elif arch == 'resnet152':\n",
    "        return models.resnet152(pretrained=pre)\n",
    "    elif arch == 'wideresnet50':\n",
    "        return models.wide_resnet50_2(pretrained=pre)\n",
    "    elif arch == 'wideresnet101':\n",
    "        return models.wide_resnet101_2(pretrained=pre)\n",
    "    elif arch == 'densenet121':\n",
    "        return models.densenet121(pretrained=pre)\n",
    "    elif arch == 'densenet169':\n",
    "        return models.densenet169(pretrained=pre)\n",
    "    elif arch == 'densenet201':\n",
    "        return models.densenet201(pretrained=pre)\n",
    "    elif arch == 'resnext50_32x4d':\n",
    "        return models.resnext50_32x4d(pretrained=pre)\n",
    "    elif arch == 'resnext101_32x8d':\n",
    "        return models.resnext101_32x8d(pretrained=pre)\n",
    "    elif arch == 'mobilenet_v2':\n",
    "        return models.mobilenet_v2(pretrained=pre)\n",
    "\n",
    "\n",
    "def modify_model(model, arch, channels, adjust):\n",
    "    count = 0\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        count += 1\n",
    "        \n",
    "    if adjust:\n",
    "        if \"densenet\" in arch:\n",
    "            in_feature = model.classifier.in_features\n",
    "            model.classifier = nn.Linear(in_features=in_feature, out_features=channels)\n",
    "        elif arch == 'mobilenet_v2':\n",
    "            in_feature = model.classifier[1].in_features\n",
    "            model.classifier = nn.Sequential(*[nn.Dropout(0.2), nn.Linear(in_feature, channels)])\n",
    "        else:\n",
    "            in_feature = model.fc.in_features\n",
    "            model.fc = nn.Linear(in_features=in_feature, out_features=channels)\n",
    "\n",
    "    return model, count\n",
    "\n",
    "def get_optimizer(optimizer_option, model, lr, wd):\n",
    "    optimizer = None\n",
    "    if optimizer_option == 'sgd':\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)\n",
    "    elif optimizer_option == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)\n",
    "    else:\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Network\n",
    "If `var_ckpt` is not None, then a model checkpoint was given. The steps to loading a neural network for continued training are : <br>\n",
    "- Create the base network.\n",
    "- Modify the base network to have the same output class.\n",
    "- Load the model checkpoint and fill in the network with the retrieved pretrained weights. \n",
    "<br>\n",
    "\n",
    "#### Important:\n",
    "In contrast to the other models the inception_v3 expects tensors with a size of N x 3 x 299 x 299, so ensure your images are sized accordingly.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRJzlPuLwMta"
   },
   "outputs": [],
   "source": [
    "if var_ckpt and os.path.exists(var_ckpt):\n",
    "    try:\n",
    "        ckpt = torch.load(var_ckpt)\n",
    "    except:\n",
    "        print('Checkpoint failed to load. Loading default network.')\n",
    "        model = get_named_model(var_arch, True)\n",
    "        model, layer_count = modify_model(model, var_arch, var_output, True)\n",
    "    else:\n",
    "        print('Checkpoint successfully loaded. Network ready for training.')\n",
    "        model, layer_count = modify_model(model, var_arch, var_output, False)\n",
    "else:\n",
    "    model = get_named_model(var_arch, True)\n",
    "    model, layer_count = modify_model(model, var_arch, var_output, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify model for multi-GPU processing\n",
    "Modify model to become a `DataParallel` class which allows a batch\n",
    "in the `DataLoader` class to be split across available GPUs in the list specified by `device_ids`. <br>\n",
    "\n",
    "Move model to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects if you have 2 GPUs and whether you specified two in system by checking the length of the gpu list.\n",
    "if len(var_gpus) > 1 and len(var_gpus) <= torch.cuda.device_count():\n",
    "    model = nn.DataParallel(model, device_ids=var_gpus)\n",
    "    print(torch.cuda.device_count())\n",
    "    \n",
    "model = model.to(var_device)\n",
    "cuda_availability = torch.cuda.is_available()\n",
    "\n",
    "if cuda_availability:\n",
    "    print('Using GPU : {}'.format(var_device))\n",
    "else:\n",
    "    print('Using CPU : {}'.format(var_device))\n",
    "    \n",
    "with open(os.path.join(proj_ckpt, '{}_arch.txt'.format(var_proj_name)), 'w') as f:\n",
    "    print(model, file=f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mxi6q05CWdBO"
   },
   "source": [
    "# Training Loop for One Epoch\n",
    "This function represents the training that occurs within one epoch or iteration. Training through all batches and images of the training dataloader represents one epoch or iteration.\n",
    "<br>\n",
    "The loss is recorded in Tensorboard. The current cumulative accuracy and loss for one epoch is also recorded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwm-7RNqZ9Sv"
   },
   "outputs": [],
   "source": [
    "def train_loop(train_loader, train_len, model, criterion, eval_criterion, optimizer, epoch, var_device):\n",
    "    t_t_start = time.time()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    comp_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    pic_count = 0\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        if cuda_availability:\n",
    "            images = images.to(var_device)\n",
    "            target = target.to(var_device)\n",
    "\n",
    "        pic_count += images.size(0)\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "        compare_loss = eval_criterion(output, target)\n",
    "    \n",
    "        \n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        comp_loss += compare_loss.item()*images.size(0)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        equals = top_class == target.view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()*images.size(0)\n",
    "        \n",
    "        current_training_loss = train_loss / pic_count\n",
    "        current_comp_loss = comp_loss / pic_count\n",
    "        current_training_accuracy = (train_accuracy / pic_count ) * 100\n",
    "\n",
    "        writer.add_scalar('weighted_ce_loss/training', current_training_loss, epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('accuracy/training', current_training_accuracy, epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('ce_loss/training', current_comp_loss, epoch * len(train_loader) + i)\n",
    "            \n",
    "        # compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        if i % 10 == 0:\n",
    "            print('{}:{}  Avg Training Loss: {:.3f}  Avg Training Accuracy: {:.8f}'.format(str(epoch), i, current_training_loss, current_training_accuracy))\n",
    "    \n",
    "    f_t_end = time.time() - t_t_start\n",
    "            \n",
    "    return train_loss, train_accuracy, comp_loss, pic_count, f_t_end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Loop for One Epoch\n",
    "Validation loop is ran after each training session in order to evaluate the performance of the neural network after training.\n",
    "<br>\n",
    "The loss is recorded in Tensorboard. The current cumulative accuracy and loss for one epoch is also recorded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IA40YZZCcomV"
   },
   "outputs": [],
   "source": [
    "def validate_loop(val_dataloader, val_len, model, criterion, eval_criterion, epoch, var_device):\n",
    "    t_t_start = time.time()\n",
    "\n",
    "    valid = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    val_pic_count = 0\n",
    "    ce = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    for i, (images, target) in enumerate(val_dataloader):\n",
    "        if cuda_availability:\n",
    "            images = images.to(var_device)\n",
    "            target = target.to(var_device)\n",
    "\n",
    "        val_pic_count += images.size(0)\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "        ce_loss = eval_criterion(output, target)\n",
    "        \n",
    "        valid += loss.item()*images.size(0)\n",
    "        ce += ce_loss.item()*images.size(0)\n",
    "        \n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        equals = top_class == target.view(*top_class.shape)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()*images.size(0)\n",
    "        current_accuracy = (val_accuracy / val_pic_count ) * 100\n",
    "        valid_avg = (valid / val_pic_count)\n",
    "\n",
    "        writer.add_scalar('weighted_ce_loss/val', valid_avg, epoch * len(val_dataloader) + i)\n",
    "        writer.add_scalar('accuracy/val', current_accuracy, epoch * len(val_dataloader) + i)\n",
    "\n",
    "        print('{}:{}  Avg Val Loss: {:.3f}  Avg Val Accuracy: {:.8f}'.format(str(epoch), i, valid_avg, current_accuracy))\n",
    "\n",
    "    f_t_end = time.time() - t_t_start\n",
    "\n",
    "    return valid, val_accuracy, ce, val_pic_count, f_t_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing Loss and Accuracy Metrics\n",
    "View and analyze the training and testing metrics for loss and accuracy. This function is called when validation loss tapers. <br>\n",
    "TODO: Save graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metric_graphs(train_loss_track, valid_loss_track, train_acc_track, valid_acc_track, epoch):\n",
    "    plt.plot(train_loss_track, label='Training loss')\n",
    "    plt.plot(valid_loss_track, label='Validation loss')\n",
    "    plt.legend(frameon=False)\n",
    "    \n",
    "    fig_loss = str(epoch) + 'loss.png'\n",
    "    fig_path = os.path.join(graph_dir, fig_loss)\n",
    "    plt.savefig(fig_path)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_acc_track, label='Training acc')\n",
    "    plt.plot(valid_acc_track, label='Validation acc')\n",
    "    plt.legend(frameon=False)\n",
    "\n",
    "    fig_acc = str(epoch) + 'acc.png'\n",
    "    fig_path = os.path.join(graph_dir, fig_acc)\n",
    "    plt.savefig(fig_path)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Predictions With Confusion Matrix\n",
    "Visualize the network's accuracy and performance on classification of the three class networks via Confusion Matrix.<br>\n",
    "TODO: Save Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(epoch):\n",
    "    test_classes = val_dataset.classes\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(len(test_classes)))\n",
    "    class_total = list(0. for i in range(len(test_classes)))\n",
    "    actual_labels = []\n",
    "    pred_labels = []\n",
    "    missclassified_imgs = []\n",
    "    model.eval()\n",
    "    for index, (data, target) in enumerate(val_dataloader):\n",
    "        if cuda_availability:\n",
    "            data = data.to(var_device)\n",
    "            target = target.to(var_device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()*data.size(0)\n",
    "\n",
    "        output = output.detach().cpu()\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "\n",
    "        target = target.cpu()\n",
    "        equals = top_class == target.view(*top_class.shape)\n",
    "\n",
    "        for i in range(len(target)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += equals[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "        misscls_idx = np.argwhere(equals == False)[0]\n",
    "        data = data.cpu()\n",
    "        for i in misscls_idx:\n",
    "            missclassified_imgs.append((data[i],target[i],top_class[i]))\n",
    "\n",
    "        actual_labels.extend(target)\n",
    "        pred_labels.extend(top_class.view(*target.shape))\n",
    "\n",
    "\n",
    "    # average test loss\n",
    "    test_loss = test_loss/len(val_dataloader.dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    cm = confusion_matrix(actual_labels,pred_labels)\n",
    "    print(cm)\n",
    "    \n",
    "    class_names = val_dataset.classes\n",
    "    if is_notebook:\n",
    "        fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
    "                                        colorbar=True,\n",
    "                                        show_absolute=True,\n",
    "                                        class_names=class_names)\n",
    "        plt.title('Epoch {} Loss {} : Confusion Matrix'.format(str(epoch), str(test_loss)))\n",
    "        plt.savefig(os.path.join(cm_dir, '{}ConfusionMatrix.png'.format(str(epoch))))\n",
    "        plt.show()\n",
    "        \n",
    "    for i in range(len(test_classes)):\n",
    "        if class_total[i] > 0:\n",
    "            recall = cm[i][i] / (cm[i][0] + cm[i][1] + cm[i][2])\n",
    "            precision = cm[i][i]/ (cm[0][i] + cm[1][i] + cm[2][i])\n",
    "        \n",
    "            f1 = (2.*precision*recall)/(precision+recall)\n",
    "            \n",
    "            print('Test Accuracy/Sensitivity of {}: {:.2f}% ({}/{})'.format(test_classes[i], 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "            print('Test Precision of {}: {:.2f}% '.format(test_classes[i], 100*precision))\n",
    "            print('Test F1 score of {}: {:.2f}% '.format(test_classes[i], 100*f1))\n",
    "        else:\n",
    "            print('Test Accuracy/Sensitivity of {}: N/A (no training examples)' % (test_classes[i]))\n",
    "        \n",
    "        \n",
    "    print('\\nTest Accuracy (Overall): {:.2f}% ({}/{})'.format(100. * np.sum(class_correct) / np.sum(class_total),\n",
    "                                                          np.sum(class_correct), np.sum(class_total))) \n",
    "    \n",
    "    return cm[0][0] / (cm[0][0] + cm[0][1] + cm[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where The Magic Happens ~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Settings and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = range(var_iter)\n",
    "optimizer = get_optimizer(var_optimizer, model, var_lr, var_weight_decay)\n",
    "\n",
    "# Training set Distribution: {'COVID-19': 175, 'normal': 7081, 'pneumonia': 4836}\n",
    "# Max(Number of occurrences in most common class) / (Number of occurrences in rare classes)\n",
    "\n",
    "covid_weight = (7081.)/(175.)\n",
    "normal_weight = (7081.)/(7081.)\n",
    "pneumonia_weight = (7081.)/(4836.)\n",
    "\n",
    "class_weights = torch.tensor([covid_weight,normal_weight,pneumonia_weight]).to(var_device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights) if var_use_focal is False else FocalLoss(var_alpha, var_gamma, var_reduction)\n",
    "#criterion = nn.CrossEntropyLoss() if var_use_focal is False else FocalLoss(var_alpha, var_gamma, var_reduction)\n",
    "eval_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = None\n",
    "if var_reduce_lr:\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=var_lr_plateau)\n",
    "    \n",
    "# Track some metrics for the current training session\n",
    "val_best_acc = 0\n",
    "val_best_loss = np.Inf\n",
    "current_index = 0\n",
    "train_loss_track = []\n",
    "valid_loss_track = []\n",
    "train_acc_track = []\n",
    "valid_acc_track = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Class Probabilities.\n",
    "Retrieves the probability output for all classes and stores the file name, class probabilities, and target in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare helper class to hold data images and file paths.\n",
    "class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n",
    "    # Extends torchvision.datasets.ImageFolder to return img and file name.\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_class_statistics(val_dataloader, test_dataloader, model, var_device, proj_ckpt, csv_file):\n",
    "    results = []\n",
    "    model.to(var_device)\n",
    "    model.eval()\n",
    "    k = 0\n",
    "    class_list = train_dataset.classes\n",
    "    columns = 'file'\n",
    "    for classes in class_list:\n",
    "        columns += ',{}'.format(str(classes))\n",
    "        k += 1\n",
    "    columns += ',target,dsplit'\n",
    "    results.append(columns)\n",
    "    \n",
    "    for i, (images, target, paths) in enumerate(val_dataloader):\n",
    "        if cuda_availability:\n",
    "            images = images.to(var_device)\n",
    "            target = target.to(var_device)\n",
    "            \n",
    "        output = model(images)\n",
    "        sm = torch.nn.Softmax(dim=1)\n",
    "        output = sm(output)\n",
    "        \n",
    "        for img in range(0, images.size(0)):\n",
    "            answer = '{}'.format(os.path.basename(paths[img]))\n",
    "            for x in range(0, output.size()[1]):\n",
    "                answer += ',{:.4f}'.format((output[img][x].item()))\n",
    "            answer += ',{},{}'.format(str(target[img].item()), 'val')\n",
    "            results.append(answer)\n",
    "\n",
    "    for i, (images, target, paths) in enumerate(test_dataloader):\n",
    "        if cuda_availability:\n",
    "            images = images.to(var_device)\n",
    "            target = target.to(var_device)\n",
    "\n",
    "        output = model(images)\n",
    "        sm = torch.nn.Softmax(dim=1)\n",
    "        output = sm(output)\n",
    "\n",
    "        for img in range(0, images.size(0)):\n",
    "            answer = '{}'.format(os.path.basename(paths[img]))\n",
    "            for x in range(0, output.size()[1]):\n",
    "                answer += ',{:.4f}'.format((output[img][x].item()))\n",
    "            answer += ',{},{}'.format(str(target[img].item()), 'test')\n",
    "            results.append(answer)\n",
    "\n",
    "    with open(csv_file, 'w') as f:\n",
    "        for line in results:\n",
    "            print(line, file=f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_confusion_matrix(model, dataloader, dataloader_type):\n",
    "    cuda_availability = torch.cuda.is_available()\n",
    "    test_classes = test_dataset.classes\n",
    "    metrics_list = []\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(len(test_classes)))\n",
    "    class_total = list(0. for i in range(len(test_classes)))\n",
    "    actual_labels = []\n",
    "    pred_labels = []\n",
    "    missclassified_imgs = []\n",
    "    model.eval()\n",
    "    for index, (data, target) in enumerate(dataloader):\n",
    "        if cuda_availability:\n",
    "            data = data.to(var_device)\n",
    "            target = target.to(var_device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        output = output.detach().cpu()\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        target = target.cpu()\n",
    "        equals = top_class == target.view(*top_class.shape)\n",
    "        for i in range(len(target)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += equals[i].item()\n",
    "            class_total[label] += 1\n",
    "        misscls_idx = np.argwhere(equals == False)[0]\n",
    "        data = data.cpu()\n",
    "        for i in misscls_idx:\n",
    "            missclassified_imgs.append((data[i],target[i],top_class[i]))\n",
    "        actual_labels.extend(target)\n",
    "        pred_labels.extend(top_class.view(*target.shape))\n",
    "    # average test loss\n",
    "    test_loss = test_loss/len(dataloader.dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "    cm = confusion_matrix(actual_labels,pred_labels)\n",
    "    print(cm)\n",
    "    class_names = test_dataset.classes\n",
    "    if is_notebook:\n",
    "        if dataloader_type == 'val':\n",
    "            fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
    "                                        colorbar=True,\n",
    "                                        show_absolute=True,\n",
    "                                        class_names=class_names)\n",
    "            plt.title('Loss {} : Val Confusion Matrix'.format(str(test_loss)))\n",
    "            plt.savefig('./code/server_checkpoints/{}/results/'.format(var_proj_name) + '{}_ValConfusionMatrix.png'.format(var_proj_name))\n",
    "            # # # plt.show()()()\n",
    "            for i in range(len(test_classes)):\n",
    "                if class_total[i] > 0:\n",
    "                    recall = cm[i][i] / (cm[i][0] + cm[i][1] + cm[i][2])\n",
    "                    precision = cm[i][i]/ (cm[0][i] + cm[1][i] + cm[2][i])\n",
    "                    f1 = (2.*precision*recall)/(precision+recall)\n",
    "                    metrics_list.append((recall,precision,f1))\n",
    "                    print('Val Accuracy/Sensitivity of {}: {:.2f}% ({}/{})'.format(test_classes[i], 100 * class_correct[i] / class_total[i],\n",
    "                                                             np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "                    print('Val Precision of {}: {:.2f}% '.format(test_classes[i], 100*precision))\n",
    "                    print('Val F1 score of {}: {:.2f}%\\n '.format(test_classes[i], 100*f1))\n",
    "                else:\n",
    "                    print('Val Accuracy/Sensitivity of {}: N/A (no training examples)' % (test_classes[i]))\n",
    "            overall_accuracy = 100. * np.sum(class_correct) / np.sum(class_total)\n",
    "            print('\\nVal Accuracy (Overall): {:.2f}% ({}/{})'.format(overall_accuracy,\n",
    "                                                          np.sum(class_correct), np.sum(class_total))) \n",
    "        elif dataloader_type == 'test':\n",
    "            fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
    "                                        colorbar=True,\n",
    "                                        show_absolute=True,\n",
    "                                        class_names=class_names)\n",
    "            plt.title('Loss {} : Test Confusion Matrix'.format(str(test_loss)))\n",
    "            plt.savefig('./code/server_checkpoints/{}/results/'.format(var_proj_name) + '{}_TestConfusionMatrix.png'.format(var_proj_name))\n",
    "            # # plt.show()()()\n",
    "            for i in range(len(test_classes)):\n",
    "                if class_total[i] > 0:\n",
    "                    recall = cm[i][i] / (cm[i][0] + cm[i][1] + cm[i][2])\n",
    "                    precision = cm[i][i]/ (cm[0][i] + cm[1][i] + cm[2][i])\n",
    "                    f1 = (2.*precision*recall)/(precision+recall)\n",
    "                    metrics_list.append((recall,precision,f1))\n",
    "                    print('Test Accuracy/Sensitivity of {}: {:.2f}% ({}/{})'.format(test_classes[i], 100 * class_correct[i] / class_total[i],\n",
    "                                                             np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "                    print('Test Precision of {}: {:.2f}% '.format(test_classes[i], 100*precision))\n",
    "                    print('Test F1 score of {}: {:.2f}%\\n '.format(test_classes[i], 100*f1))\n",
    "                else:\n",
    "                    print('Test Accuracy/Sensitivity of {}: N/A (no training examples)' % (test_classes[i]))\n",
    "            overall_accuracy = 100. * np.sum(class_correct) / np.sum(class_total)\n",
    "            print('\\nTest Accuracy (Overall): {:.2f}% ({}/{})'.format(overall_accuracy,\n",
    "                                                          np.sum(class_correct), np.sum(class_total)))\n",
    "    return test_loss, metrics_list, overall_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "### Transfer Learning Protocol j\n",
    "Freeze all weights except for the last fully connected and fit the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_len = len(train_dataloader)\n",
    "val_len = len(val_dataloader)\n",
    "current_best_model = model.state_dict()\n",
    "current_best_index = 0\n",
    "logs = []\n",
    "index = 0\n",
    "stage_count = 0\n",
    "last_improved = 0\n",
    "logs_df = pd.DataFrame(columns=['epoch', 'train_time', 'train_wce', 'train_ce', 'train_accuracy', 'val_time', 'val_wce', 'val_ce', 'val_accuracy'])\n",
    "\n",
    "while index < var_iter:\n",
    "    i = index + var_start_iter\n",
    "    \n",
    "    # n-stage default: 3-stage Transfer Learning Protocol\n",
    "    stage_limit = var_stage_limit\n",
    "    \n",
    "    # Training has finished\n",
    "    if i > var_iter or last_improved == var_end_plateau or stage_count == stage_limit:\n",
    "        print('Training has completed. Evaluating network performance on test set. Logging network outputs.')\n",
    "        break\n",
    "    \n",
    "    if var_use_staged_tt:\n",
    "        # n-stage Transfer Learning Protocol\n",
    "        if last_improved == var_stage_plateau:\n",
    "            last_improved = 0\n",
    "            if is_notebook:\n",
    "                create_metric_graphs(train_loss_track, valid_loss_track, train_acc_track, valid_acc_track, index)\n",
    "\n",
    "            print('\\nStage-{} Staged Transfer Training Unfreezing\\n'.format(str(stage_count)))\n",
    "            unfrozen_factor = stage_count * var_network_ratio\n",
    "            begin_unfrozen = layer_count - (math.floor(unfrozen_factor * layer_count))\n",
    "            begin_unfrozen = 0 if begin_unfrozen < 0 else begin_unfrozen\n",
    "            cur_count = 0\n",
    "\n",
    "            for param in model.parameters():\n",
    "                if cur_count >= begin_unfrozen:\n",
    "                    param.requires_grad = True\n",
    "                cur_count += 1\n",
    "         \n",
    "            change_factor = var_lr_factor**stage_count\n",
    "\n",
    "            if var_change_style == 'steady':\n",
    "                temp_lr = var_lr + change_factor if var_use_staged_tt else var_lr + var_lr_factor\n",
    "            elif var_change_style == 'compounding':\n",
    "                temp_lr = var_lr * change_factor if var_use_staged_tt else var_lr * var_lr_factor\n",
    "\n",
    "            model = model.load_state_dict(current_best_model)\n",
    "            index = current_best_index\n",
    "            optimizer = get_optimizer(var_optimizer, model, temp_lr, var_weight_decay)\n",
    "            stage_count += 1\n",
    "\n",
    "            print('Starting training at {} with updated learning rate {}'.format(str(index), str(temp_lr)))\n",
    "        \n",
    "    # Run through the training and validation loop and capture returned metrics.\n",
    "    train_loss, train_accuracy, train_ce, pic_count, train_time = train_loop(train_dataloader, train_len, model, criterion, eval_criterion, optimizer, index, var_device)\n",
    "    valid_loss, val_accuracy, val_ce, val_pic_count, val_time = validate_loop(val_dataloader, val_len, model, criterion, eval_criterion, index, var_device)\n",
    "\n",
    "    train_loss = train_loss/pic_count\n",
    "    train_accuracy = train_accuracy/pic_count\n",
    "    train_ce = train_ce/pic_count\n",
    "    valid_loss = valid_loss/val_pic_count\n",
    "    val_accuracy = val_accuracy/val_pic_count\n",
    "    val_ce = val_ce/val_pic_count\n",
    "\n",
    "    val_acc_is_best = val_best_acc <= val_accuracy\n",
    "    val_best_acc = max(val_accuracy, val_best_acc)\n",
    "\n",
    "    val_loss_is_best = valid_loss <= val_best_loss\n",
    "    val_best_loss = min(val_best_loss, valid_loss)\n",
    "\n",
    "    train_loss_track.append(train_loss)\n",
    "    valid_loss_track.append(valid_loss)\n",
    "    train_acc_track.append(train_accuracy)\n",
    "    valid_acc_track.append(val_accuracy)\n",
    "    \n",
    "    # Update LR Scheduler based on past and current last.\n",
    "    if scheduler:\n",
    "        scheduler.step(valid_loss)\n",
    "    \n",
    "    print('Epoch: {}  Training Time: {:.4f}  Training Loss: {:.4f}  Training CE: {:.4f}  Training Accuracy: {:.4f}           Val Time: {:.4f}  Validation Loss: {:.4f}  Validation CE: {:.4f}  Validation Accuracy: {:.4f}'.format(index, train_time, train_loss, train_ce, train_accuracy, val_time, valid_loss, val_ce, val_accuracy))\n",
    "    \n",
    "    logs_df = logs_df.append({'epoch': index, 'train_time': train_time, 'train_wce': train_loss, 'train_ce': train_ce, 'train_accuracy': train_accuracy,\n",
    "                              'val_time': val_time, 'val_wce': valid_loss, 'val_ce': val_ce, 'val_accuracy': val_accuracy}, ignore_index=True)\n",
    "\n",
    "    # Save the model if it achieves a lower validation loss on the test set.\n",
    "    if val_loss_is_best:\n",
    "        current_best_model = model.state_dict()\n",
    "        current_best_index = index\n",
    "        print('Found New Val Loss Low')\n",
    "        covid_sensitivity = create_confusion_matrix(index)\n",
    "        if len(var_gpus) > 1:\n",
    "            generic_state = model.module\n",
    "        else:\n",
    "            generic_state = model\n",
    "        ext = './checkpoints/{}-{}model_bestLOSS{}.pt'.format(str(var_proj_name), str(index).split('.')[0], val_best_loss)\n",
    "            \n",
    "        print('Saving checkpoint {}'.format(ext))\n",
    "        torch.save(generic_state, ext)\n",
    "        last_improved = 0\n",
    "        \n",
    "    elif val_acc_is_best:\n",
    "        print('Found New Val Acc High')\n",
    "        covid_sensitivity=create_confusion_matrix(index)\n",
    "\n",
    "    last_improved += 1\n",
    "    index += 1\n",
    "    \n",
    "logs_df.to_csv('./checkpoints/{}_trainlogs.csv'.format(str(var_proj_name)), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(columns=['experiment_name', 'model', 'best_epoch', 'val_loss',\n",
    "                                'val_covid_recall', 'val_covid_precision', 'val_covid_f1', \n",
    "                                'val_normal_recall', 'val_normal_precision', 'val_normal_f1',\n",
    "                                'val_pneumonia_recall', 'val_pneumonia_precision', 'val_pneumonia_f1', 'val_overall_accuracy',\n",
    "                                'test_loss', 'test_covid_recall', 'test_covid_precision', 'test_covid_f1', \n",
    "                                'test_normal_recall', 'test_normal_precision', 'test_normal_f1',\n",
    "                               'test_pneumonia_recall', 'test_pneumonia_precision', 'test_pneumonia_f1', 'test_overall_accuracy'])\n",
    "train_len = len(train_dataloader)\n",
    "val_len = len(val_dataloader)\n",
    "\n",
    "model = model.load_state_dict(current_best_model)\n",
    "\n",
    "val_loss, val_metrics_list, val_overall_accuracy = eval_confusion_matrix(model, val_dataloader, dataloader_type='val')\n",
    "eval_val_dataset = ImageFolderWithPaths(val_path, transform=val_transform)\n",
    "eval_val_dataloader = data.DataLoader(\n",
    "            eval_val_dataset, \n",
    "            batch_size=var_batch_size,\n",
    "            sampler=data_sampler(eval_val_dataset, shuffle=True, distributed=var_distributed),\n",
    "        )\n",
    "test_loss, test_metrics_list, test_overall_accuracy = eval_confusion_matrix(model, test_dataloader, dataloader_type='test')\n",
    "eval_test_dataset = ImageFolderWithPaths(test_path, transform=test_transform)\n",
    "eval_test_dataloader = data.DataLoader(\n",
    "            eval_test_dataset, \n",
    "            batch_size=var_batch_size,\n",
    "            sampler=data_sampler(eval_test_dataset, shuffle=True, distributed=var_distributed),\n",
    "        )\n",
    "csv_file = './checkpoints/{}_network_output.csv'.format(var_proj_name)\n",
    "output_class_statistics(eval_val_dataloader, eval_test_dataloader, model, var_device, csv_file)\n",
    "metrics_df = metrics_df.append({'experiment_name': var_proj_name, 'model': var_arch, 'best_epoch': index, 'val_loss': val_loss, \n",
    "                          'val_covid_recall': val_metrics_list[0][0], 'val_covid_precision': val_metrics_list[0][1],\n",
    "                          'val_covid_f1': val_metrics_list[0][2], 'val_normal_recall': val_metrics_list[1][0], \n",
    "                          'val_normal_precision': val_metrics_list[1][1], 'val_normal_f1':val_metrics_list[1][2],\n",
    "                          'val_pneumonia_recall': val_metrics_list[2][0], 'val_pneumonia_precision': val_metrics_list[2][1],\n",
    "                          'val_pneumonia_f1': val_metrics_list[2][2], 'val_overall_accuracy': val_overall_accuracy,\n",
    "                          'test_loss': test_loss, 'test_covid_recall': test_metrics_list[0][0], \n",
    "                          'test_covid_precision':test_metrics_list[0][1], 'test_covid_f1':test_metrics_list[0][2], \n",
    "                          'test_normal_recall': test_metrics_list[1][0], 'test_normal_precision': test_metrics_list[1][1], \n",
    "                          'test_normal_f1':test_metrics_list[1][2], 'test_pneumonia_recall': test_metrics_list[2][0], \n",
    "                          'test_pneumonia_precision': test_metrics_list[2][1], 'test_pneumonia_f1': test_metrics_list[2][2], \n",
    "                          'test_overall_accuracy': test_overall_accuracy}, ignore_index=True)\n",
    "metrics_df.to_csv('./checkpoints/{}_metrics.csv'.format(var_proj_name),index=False)\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid19Demo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
